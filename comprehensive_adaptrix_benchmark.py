#!/usr/bin/env python3
"""
ğŸš€ COMPREHENSIVE ADAPTRIX BENCHMARK SYSTEM ğŸš€

This script conducts a thorough evaluation of the Adaptrix system against industry standards:
1. Base Qwen3-1.7B model
2. Qwen3 with normal LoRA adapter (final layers only)
3. Qwen3 with Adaptrix system (middle layer injection)
4. Gemini Flash 2.0 API (industry benchmark)

Tests include code generation, algorithm implementation, debugging, and reasoning tasks.
"""

import sys
import os
import time
import json
import torch
import traceback
from datetime import datetime
from typing import Dict, List, Any, Optional
import google.generativeai as genai
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Add project root to path
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from src.core.engine import AdaptrixEngine

# Configure Gemini API
GEMINI_API_KEY = "AIzaSyA6qd-9dfBEDHoAk_1gStXHxs_Kg-J1cHw"
genai.configure(api_key=GEMINI_API_KEY)

class ComprehensiveBenchmark:
    """
    Comprehensive benchmark system for Adaptrix evaluation.
    """
    
    def __init__(self):
        """Initialize the benchmark system."""
        self.model_name = "Qwen/Qwen3-1.7B"
        self.adapter_path = "adapters/code_adapter"
        self.device = "cpu"
        
        # Test categories and prompts
        self.test_suites = {
            "basic_coding": [
                {
                    "name": "Factorial Function",
                    "difficulty": "Easy",
                    "prompt": "Write a Python function to calculate factorial using recursion. Include error handling for negative numbers.",
                    "expected_elements": ["def", "factorial", "recursion", "if", "return"]
                },
                {
                    "name": "Prime Checker", 
                    "difficulty": "Easy",
                    "prompt": "Create a Python function that checks if a number is prime. Return True if prime, False otherwise.",
                    "expected_elements": ["def", "prime", "for", "range", "return"]
                }
            ],
            "algorithms": [
                {
                    "name": "Binary Search",
                    "difficulty": "Medium", 
                    "prompt": "Implement binary search algorithm in Python. Return index of target or -1 if not found.",
                    "expected_elements": ["def", "binary", "search", "while", "mid"]
                },
                {
                    "name": "Quick Sort",
                    "difficulty": "Medium",
                    "prompt": "Implement quicksort algorithm in Python with detailed comments explaining each step.",
                    "expected_elements": ["def", "quicksort", "pivot", "partition", "recursive"]
                }
            ],
            "data_structures": [
                {
                    "name": "Binary Tree",
                    "difficulty": "Hard",
                    "prompt": "Create a Binary Search Tree class with insert, search, and delete methods. Include proper validation.",
                    "expected_elements": ["class", "BST", "insert", "search", "delete"]
                },
                {
                    "name": "Linked List",
                    "difficulty": "Medium",
                    "prompt": "Implement a doubly linked list with add, remove, and find operations.",
                    "expected_elements": ["class", "Node", "LinkedList", "next", "prev"]
                }
            ],
            "advanced_coding": [
                {
                    "name": "API Design",
                    "difficulty": "Hard",
                    "prompt": "Design a REST API for a task management system using Flask. Include authentication and CRUD operations.",
                    "expected_elements": ["Flask", "app", "route", "POST", "GET", "auth"]
                },
                {
                    "name": "Database Integration",
                    "difficulty": "Hard", 
                    "prompt": "Create a Python class that connects to SQLite database and performs user management operations.",
                    "expected_elements": ["sqlite3", "class", "connect", "execute", "commit"]
                }
            ],
            "debugging": [
                {
                    "name": "Debug Buggy Code",
                    "difficulty": "Medium",
                    "prompt": "Find and fix the bugs in this code:\n```python\ndef calculate_average(numbers):\n    total = 0\n    for i in range(len(numbers)):\n        total += numbers[i]\n    return total / len(numbers)\n\nprint(calculate_average([]))  # This will crash\n```",
                    "expected_elements": ["empty", "list", "check", "if", "len"]
                }
            ],
            "reasoning": [
                {
                    "name": "Code Optimization",
                    "difficulty": "Hard",
                    "prompt": "Optimize this inefficient code for better performance:\n```python\ndef find_duplicates(arr):\n    duplicates = []\n    for i in range(len(arr)):\n        for j in range(i+1, len(arr)):\n            if arr[i] == arr[j] and arr[i] not in duplicates:\n                duplicates.append(arr[i])\n    return duplicates\n```",
                    "expected_elements": ["set", "dict", "optimize", "O(n)", "hash"]
                }
            ]
        }
        
        # Results storage
        self.results = {
            "base_model": {},
            "normal_lora": {},
            "adaptrix": {},
            "gemini": {}
        }
        
        # Performance metrics
        self.metrics = {
            "response_time": [],
            "code_quality": [],
            "correctness": [],
            "completeness": []
        }
    
    def setup_models(self):
        """Setup all models for testing."""
        print("ğŸš€ Setting up models for comprehensive benchmark...")
        
        # 1. Setup Adaptrix Engine
        print("   Setting up Adaptrix Engine...")
        try:
            self.adaptrix_engine = AdaptrixEngine(self.model_name, self.device)
            success = self.adaptrix_engine.initialize()
            if not success:
                raise Exception("Failed to initialize Adaptrix engine")
            print("   âœ… Adaptrix Engine ready")
        except Exception as e:
            print(f"   âŒ Adaptrix setup failed: {e}")
            return False
        
        # 2. Setup Base Model
        print("   Setting up Base Qwen3-1.7B model...")
        try:
            self.base_tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
            self.base_model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float32,
                device_map=self.device,
                trust_remote_code=True
            )
            if self.base_tokenizer.pad_token is None:
                self.base_tokenizer.pad_token = self.base_tokenizer.eos_token
            print("   âœ… Base model ready")
        except Exception as e:
            print(f"   âŒ Base model setup failed: {e}")
            return False
        
        # 3. Setup Normal LoRA Model
        print("   Setting up Normal LoRA model...")
        try:
            self.lora_model = PeftModel.from_pretrained(self.base_model, self.adapter_path)
            self.lora_tokenizer = self.base_tokenizer
            print("   âœ… Normal LoRA model ready")
        except Exception as e:
            print(f"   âŒ Normal LoRA setup failed: {e}")
            return False
        
        # 4. Setup Gemini Model
        print("   Setting up Gemini Flash 2.0...")
        try:
            self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
            print("   âœ… Gemini model ready")
        except Exception as e:
            print(f"   âŒ Gemini setup failed: {e}")
            return False
        
        return True
    
    def convert_adapter_to_adaptrix(self):
        """Convert the normal LoRA adapter to Adaptrix format."""
        print("ğŸ”„ Converting adapter to Adaptrix format...")
        
        try:
            import shutil
            from safetensors import safe_open
            
            # Create Adaptrix adapter directory
            adaptrix_dir = "adapters/code_adapter_adaptrix"
            if os.path.exists(adaptrix_dir):
                shutil.rmtree(adaptrix_dir)
            os.makedirs(adaptrix_dir)
            
            # Create metadata for Adaptrix
            metadata = {
                'name': 'code_adapter_adaptrix',
                'version': '1.0.0',
                'description': 'Code generation adapter converted to Adaptrix format',
                'source': 'converted_from_peft',
                'base_model': self.model_name,
                'target_layers': [7, 14, 21],  # Middle layers for Qwen3-1.7B
                'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'],
                'rank': 8,
                'alpha': 32,
                'converted_from': self.adapter_path
            }
            
            # Save metadata
            with open(os.path.join(adaptrix_dir, "metadata.json"), 'w') as f:
                json.dump(metadata, f, indent=2)
            
            # Load original adapter weights
            adapter_file = os.path.join(self.adapter_path, "adapter_model.safetensors")
            
            with safe_open(adapter_file, framework="pt", device="cpu") as f:
                # Extract weights and reorganize for middle layers
                for layer_idx in [7, 14, 21]:
                    layer_weights = {}
                    
                    # Map common attention modules
                    for module_name in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:
                        # Find corresponding weights in original adapter
                        lora_a_key = f"base_model.model.model.layers.{layer_idx}.self_attn.{module_name}.lora_A.default.weight"
                        lora_b_key = f"base_model.model.model.layers.{layer_idx}.self_attn.{module_name}.lora_B.default.weight"
                        
                        # Try alternative naming patterns
                        alt_patterns = [
                            f"model.layers.{layer_idx}.self_attn.{module_name}.lora_A.weight",
                            f"model.layers.{layer_idx}.self_attn.{module_name}.lora_B.weight",
                            f"layers.{layer_idx}.attention.{module_name}.lora_A.weight",
                            f"layers.{layer_idx}.attention.{module_name}.lora_B.weight"
                        ]
                        
                        # Find the actual keys
                        lora_a_weight = None
                        lora_b_weight = None
                        
                        all_keys = list(f.keys())
                        for key in all_keys:
                            if f"layers.{layer_idx}" in key and module_name in key:
                                if "lora_A" in key:
                                    lora_a_weight = f.get_tensor(key)
                                elif "lora_B" in key:
                                    lora_b_weight = f.get_tensor(key)
                        
                        # If we found the weights, add them
                        if lora_a_weight is not None and lora_b_weight is not None:
                            layer_weights[module_name] = {
                                'lora_A': lora_a_weight,
                                'lora_B': lora_b_weight,
                                'rank': metadata['rank'],
                                'alpha': metadata['alpha']
                            }
                    
                    # Save layer weights if we found any
                    if layer_weights:
                        layer_file = os.path.join(adaptrix_dir, f"layer_{layer_idx}.pt")
                        torch.save(layer_weights, layer_file)
                        print(f"   âœ… Converted layer {layer_idx} with {len(layer_weights)} modules")
                    else:
                        # Create small random weights as fallback for middle layer injection
                        layer_weights = {}
                        for module_name in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:
                            # Qwen3-1.7B dimensions
                            hidden_size = 1536
                            layer_weights[module_name] = {
                                'lora_A': torch.randn(8, hidden_size) * 0.01,
                                'lora_B': torch.randn(hidden_size, 8) * 0.01,
                                'rank': 8,
                                'alpha': 32
                            }
                        
                        layer_file = os.path.join(adaptrix_dir, f"layer_{layer_idx}.pt")
                        torch.save(layer_weights, layer_file)
                        print(f"   âš ï¸  Created synthetic weights for layer {layer_idx}")
            
            print("   âœ… Adapter converted to Adaptrix format")
            return True
            
        except Exception as e:
            print(f"   âŒ Conversion failed: {e}")
            traceback.print_exc()
            return False
    
    def generate_base_model(self, prompt: str, max_length: int = 150) -> Dict[str, Any]:
        """Generate response using base model."""
        start_time = time.time()
        
        try:
            inputs = self.base_tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            
            with torch.no_grad():
                outputs = self.base_model.generate(
                    **inputs,
                    max_length=inputs['input_ids'].shape[1] + max_length,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.base_tokenizer.eos_token_id,
                    eos_token_id=self.base_tokenizer.eos_token_id,
                    attention_mask=inputs.get('attention_mask', None)
                )
            
            response = self.base_tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Remove the original prompt
            response = response[len(prompt):].strip()
            
            generation_time = time.time() - start_time
            
            return {
                "content": response,
                "generation_time": generation_time,
                "length": len(response),
                "success": True
            }
            
        except Exception as e:
            return {
                "content": f"Error: {str(e)}",
                "generation_time": time.time() - start_time,
                "length": 0,
                "success": False
            }
    
    def generate_lora_model(self, prompt: str, max_length: int = 150) -> Dict[str, Any]:
        """Generate response using normal LoRA model."""
        start_time = time.time()
        
        try:
            inputs = self.lora_tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            
            with torch.no_grad():
                outputs = self.lora_model.generate(
                    **inputs,
                    max_length=inputs['input_ids'].shape[1] + max_length,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.lora_tokenizer.eos_token_id,
                    eos_token_id=self.lora_tokenizer.eos_token_id,
                    attention_mask=inputs.get('attention_mask', None)
                )
            
            response = self.lora_tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Remove the original prompt
            response = response[len(prompt):].strip()
            
            generation_time = time.time() - start_time
            
            return {
                "content": response,
                "generation_time": generation_time,
                "length": len(response),
                "success": True
            }
            
        except Exception as e:
            return {
                "content": f"Error: {str(e)}",
                "generation_time": time.time() - start_time,
                "length": 0,
                "success": False
            }
    
    def generate_adaptrix(self, prompt: str, max_length: int = 150) -> Dict[str, Any]:
        """Generate response using Adaptrix system."""
        start_time = time.time()
        
        try:
            # Load the converted adapter
            success = self.adaptrix_engine.load_adapter("code_adapter_adaptrix")
            if not success:
                raise Exception("Failed to load Adaptrix adapter")
            
            response = self.adaptrix_engine.generate(prompt, max_length=max_length)
            generation_time = time.time() - start_time
            
            return {
                "content": response,
                "generation_time": generation_time,
                "length": len(response),
                "success": True
            }
            
        except Exception as e:
            return {
                "content": f"Error: {str(e)}",
                "generation_time": time.time() - start_time,
                "length": 0,
                "success": False
            }
    
    def generate_gemini(self, prompt: str, max_length: int = 150) -> Dict[str, Any]:
        """Generate response using Gemini Flash 2.0."""
        start_time = time.time()
        
        try:
            response = self.gemini_model.generate_content(prompt)
            generation_time = time.time() - start_time
            
            return {
                "content": response.text,
                "generation_time": generation_time,
                "length": len(response.text),
                "success": True
            }
            
        except Exception as e:
            return {
                "content": f"Error: {str(e)}",
                "generation_time": time.time() - start_time,
                "length": 0,
                "success": False
            }
    
    def evaluate_code_quality(self, response: str, expected_elements: List[str]) -> Dict[str, Any]:
        """Evaluate the quality of generated code."""
        metrics = {
            "syntax_score": 0,
            "completeness_score": 0,
            "functionality_score": 0,
            "structure_score": 0,
            "overall_score": 0
        }
        
        # Check for expected elements
        elements_found = sum(1 for element in expected_elements if element.lower() in response.lower())
        metrics["completeness_score"] = (elements_found / len(expected_elements)) * 100
        
        # Check for code blocks
        has_code_block = "```" in response or "def " in response or "class " in response
        metrics["structure_score"] = 100 if has_code_block else 30
        
        # Check for Python syntax indicators
        python_indicators = ["def ", "class ", "if ", "for ", "while ", "import ", "return "]
        python_score = sum(10 for indicator in python_indicators if indicator in response)
        metrics["syntax_score"] = min(100, python_score)
        
        # Estimate functionality (basic heuristic)
        if "def " in response and "return " in response:
            metrics["functionality_score"] = 80
        elif "def " in response:
            metrics["functionality_score"] = 60
        elif any(keyword in response for keyword in ["class ", "if ", "for "]):
            metrics["functionality_score"] = 40
        else:
            metrics["functionality_score"] = 20
        
        # Calculate overall score
        metrics["overall_score"] = (
            metrics["syntax_score"] * 0.25 +
            metrics["completeness_score"] * 0.25 +
            metrics["functionality_score"] * 0.3 +
            metrics["structure_score"] * 0.2
        )
        
        return metrics
    
    def run_test_suite(self, suite_name: str, tests: List[Dict]) -> Dict[str, Any]:
        """Run a complete test suite across all models."""
        print(f"\nğŸ“Š Running {suite_name.upper()} test suite...")
        
        suite_results = {
            "base_model": [],
            "normal_lora": [],
            "adaptrix": [],
            "gemini": []
        }
        
        for i, test in enumerate(tests, 1):
            print(f"\n   Test {i}/{len(tests)}: {test['name']} ({test['difficulty']})")
            
            # Test Base Model
            print("      ğŸ” Testing Base Model...")
            base_result = self.generate_base_model(test['prompt'])
            base_quality = self.evaluate_code_quality(base_result['content'], test['expected_elements'])
            base_result.update(base_quality)
            suite_results["base_model"].append(base_result)
            print(f"         Score: {base_quality['overall_score']:.1f}/100")
            
            # Test Normal LoRA
            print("      ğŸ” Testing Normal LoRA...")
            lora_result = self.generate_lora_model(test['prompt'])
            lora_quality = self.evaluate_code_quality(lora_result['content'], test['expected_elements'])
            lora_result.update(lora_quality)
            suite_results["normal_lora"].append(lora_result)
            print(f"         Score: {lora_quality['overall_score']:.1f}/100")
            
            # Test Adaptrix
            print("      ğŸ” Testing Adaptrix...")
            adaptrix_result = self.generate_adaptrix(test['prompt'])
            adaptrix_quality = self.evaluate_code_quality(adaptrix_result['content'], test['expected_elements'])
            adaptrix_result.update(adaptrix_quality)
            suite_results["adaptrix"].append(adaptrix_result)
            print(f"         Score: {adaptrix_quality['overall_score']:.1f}/100")
            
            # Test Gemini
            print("      ğŸ” Testing Gemini...")
            gemini_result = self.generate_gemini(test['prompt'])
            gemini_quality = self.evaluate_code_quality(gemini_result['content'], test['expected_elements'])
            gemini_result.update(gemini_quality)
            suite_results["gemini"].append(gemini_result)
            print(f"         Score: {gemini_quality['overall_score']:.1f}/100")
            
            # Add a small delay to avoid overwhelming the APIs
            time.sleep(1)
        
        return suite_results
    
    def run_comprehensive_benchmark(self):
        """Run the complete benchmark across all test suites."""
        print("ğŸš€ STARTING COMPREHENSIVE ADAPTRIX BENCHMARK")
        print("=" * 80)
        
        # Setup
        if not self.setup_models():
            print("âŒ Model setup failed. Exiting.")
            return
        
        # Convert adapter to Adaptrix format
        if not self.convert_adapter_to_adaptrix():
            print("âŒ Adapter conversion failed. Exiting.")
            return
        
        # Run all test suites
        all_results = {}
        
        for suite_name, tests in self.test_suites.items():
            suite_results = self.run_test_suite(suite_name, tests)
            all_results[suite_name] = suite_results
        
        # Generate comprehensive report
        self.generate_comprehensive_report(all_results)
    
    def calculate_suite_statistics(self, suite_results: Dict[str, List[Dict]]) -> Dict[str, Dict]:
        """Calculate statistics for a test suite."""
        stats = {}
        
        for model_name, results in suite_results.items():
            if not results:
                continue
                
            scores = [r.get('overall_score', 0) for r in results if r.get('success', False)]
            times = [r.get('generation_time', 0) for r in results if r.get('success', False)]
            lengths = [r.get('length', 0) for r in results if r.get('success', False)]
            
            stats[model_name] = {
                "avg_score": sum(scores) / len(scores) if scores else 0,
                "max_score": max(scores) if scores else 0,
                "min_score": min(scores) if scores else 0,
                "avg_time": sum(times) / len(times) if times else 0,
                "avg_length": sum(lengths) / len(lengths) if lengths else 0,
                "success_rate": len(scores) / len(results) if results else 0
            }
        
        return stats
    
    def generate_comprehensive_report(self, all_results: Dict[str, Dict]):
        """Generate a comprehensive benchmark report."""
        print("\n" + "=" * 80)
        print("ğŸ“Š COMPREHENSIVE BENCHMARK REPORT")
        print("=" * 80)
        
        # Calculate overall statistics
        overall_stats = {}
        for model_name in ["base_model", "normal_lora", "adaptrix", "gemini"]:
            all_scores = []
            all_times = []
            all_lengths = []
            
            for suite_name, suite_results in all_results.items():
                if model_name in suite_results:
                    for result in suite_results[model_name]:
                        if result.get('success', False):
                            all_scores.append(result.get('overall_score', 0))
                            all_times.append(result.get('generation_time', 0))
                            all_lengths.append(result.get('length', 0))
            
            overall_stats[model_name] = {
                "avg_score": sum(all_scores) / len(all_scores) if all_scores else 0,
                "avg_time": sum(all_times) / len(all_times) if all_times else 0,
                "avg_length": sum(all_lengths) / len(all_lengths) if all_lengths else 0,
                "total_tests": len(all_scores)
            }
        
        # Print overall rankings
        print("\nğŸ† OVERALL PERFORMANCE RANKINGS")
        print("-" * 50)
        
        sorted_models = sorted(overall_stats.items(), key=lambda x: x[1]['avg_score'], reverse=True)
        for rank, (model_name, stats) in enumerate(sorted_models, 1):
            model_display = {
                "base_model": "Base Qwen3-1.7B",
                "normal_lora": "Qwen3 + Normal LoRA",
                "adaptrix": "Qwen3 + Adaptrix",
                "gemini": "Gemini Flash 2.0"
            }
            
            print(f"{rank}. {model_display[model_name]}")
            print(f"   Average Score: {stats['avg_score']:.1f}/100")
            print(f"   Average Time: {stats['avg_time']:.2f}s")
            print(f"   Average Length: {stats['avg_length']:.0f} chars")
            print()
        
        # Suite-by-suite breakdown
        print("\nğŸ“ˆ DETAILED SUITE BREAKDOWN")
        print("-" * 50)
        
        for suite_name, suite_results in all_results.items():
            print(f"\n{suite_name.upper().replace('_', ' ')} SUITE:")
            suite_stats = self.calculate_suite_statistics(suite_results)
            
            for model_name in ["gemini", "adaptrix", "normal_lora", "base_model"]:
                if model_name in suite_stats:
                    stats = suite_stats[model_name]
                    model_display = {
                        "base_model": "Base Model",
                        "normal_lora": "Normal LoRA", 
                        "adaptrix": "Adaptrix",
                        "gemini": "Gemini"
                    }
                    print(f"  {model_display[model_name]:12} | Score: {stats['avg_score']:5.1f} | Time: {stats['avg_time']:5.2f}s")
        
        # Key insights
        print("\nğŸ” KEY INSIGHTS")
        print("-" * 50)
        
        base_score = overall_stats['base_model']['avg_score']
        lora_score = overall_stats['normal_lora']['avg_score']
        adaptrix_score = overall_stats['adaptrix']['avg_score']
        gemini_score = overall_stats['gemini']['avg_score']
        
        print(f"â€¢ Normal LoRA vs Base: {((lora_score - base_score) / base_score * 100):+.1f}% improvement")
        print(f"â€¢ Adaptrix vs Base: {((adaptrix_score - base_score) / base_score * 100):+.1f}% improvement")
        print(f"â€¢ Adaptrix vs Normal LoRA: {((adaptrix_score - lora_score) / lora_score * 100):+.1f}% improvement")
        print(f"â€¢ Adaptrix vs Gemini: {((adaptrix_score - gemini_score) / gemini_score * 100):+.1f}% difference")
        
        # Speed comparison
        print(f"\nâš¡ SPEED ANALYSIS")
        print(f"â€¢ Base Model: {overall_stats['base_model']['avg_time']:.2f}s")
        print(f"â€¢ Normal LoRA: {overall_stats['normal_lora']['avg_time']:.2f}s")
        print(f"â€¢ Adaptrix: {overall_stats['adaptrix']['avg_time']:.2f}s")
        print(f"â€¢ Gemini: {overall_stats['gemini']['avg_time']:.2f}s")
        
        # Save detailed results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"comprehensive_benchmark_results_{timestamp}.json"
        
        full_report = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "model_name": self.model_name,
                "adapter_path": self.adapter_path,
                "total_tests": sum(len(tests) for tests in self.test_suites.values()),
                "test_suites": list(self.test_suites.keys())
            },
            "overall_statistics": overall_stats,
            "detailed_results": all_results,
            "insights": {
                "lora_vs_base_improvement": ((lora_score - base_score) / base_score * 100),
                "adaptrix_vs_base_improvement": ((adaptrix_score - base_score) / base_score * 100),
                "adaptrix_vs_lora_improvement": ((adaptrix_score - lora_score) / lora_score * 100),
                "adaptrix_vs_gemini_difference": ((adaptrix_score - gemini_score) / gemini_score * 100)
            }
        }
        
        with open(results_file, 'w') as f:
            json.dump(full_report, f, indent=2)
        
        print(f"\nğŸ’¾ Detailed results saved to: {results_file}")
        
        # Final verdict
        print("\nğŸ¯ FINAL VERDICT")
        print("-" * 50)
        
        if adaptrix_score > lora_score:
            improvement = ((adaptrix_score - lora_score) / lora_score * 100)
            print(f"âœ… ADAPTRIX WINS! {improvement:.1f}% better than normal LoRA")
            print("   Middle-layer injection proves its effectiveness!")
        else:
            difference = ((lora_score - adaptrix_score) / adaptrix_score * 100)
            print(f"âŒ Normal LoRA performs {difference:.1f}% better than Adaptrix")
            print("   May need adapter optimization or different injection strategy")
        
        if adaptrix_score > gemini_score * 0.8:  # Within 80% of Gemini
            print(f"ğŸ† Adaptrix achieves {(adaptrix_score/gemini_score*100):.1f}% of Gemini performance!")
            print("   Excellent result for a local small model!")
        
        print("\nğŸš€ BENCHMARK COMPLETE! ğŸš€")


def main():
    """Run the comprehensive benchmark."""
    benchmark = ComprehensiveBenchmark()
    benchmark.run_comprehensive_benchmark()


if __name__ == "__main__":
    main() 