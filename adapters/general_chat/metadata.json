{
  "name": "converted_peft_adapter",
  "version": "1.0.0",
  "description": "Converted from PEFT adapter for microsoft/DialoGPT-small",
  "source": "peft_conversion",
  "base_model": "microsoft/DialoGPT-small",
  "target_layers": [
    3,
    6,
    9
  ],
  "target_modules": [
    "attn.c_attn",
    "attn.c_proj",
    "mlp.c_fc",
    "mlp.c_proj"
  ],
  "rank": 16,
  "alpha": 16,
  "original_peft_config": "LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='decapoda-research/llama-7b-hf', revision=None, inference_mode=True, r=16, target_modules={'k_proj', 'v_proj', 'o_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
}