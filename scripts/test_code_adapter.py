#!/usr/bin/env python3
"""
ğŸ§ª CODE ADAPTER COMPREHENSIVE TESTING

Tests the new "code" adapter in the adapters folder with Qwen3-1.7B model.
Performs detailed evaluation of code generation capabilities.
"""

import sys
import os
import time
import json
from pathlib import Path

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)


def inspect_code_adapter():
    """Inspect the code adapter configuration."""
    
    print("ğŸ” INSPECTING CODE ADAPTER")
    print("=" * 60)
    
    adapter_path = Path("adapters/code")
    
    if not adapter_path.exists():
        print(f"âŒ Code adapter not found at {adapter_path}")
        return None
    
    # Check adapter files
    config_file = adapter_path / "adapter_config.json"
    metadata_file = adapter_path / "adaptrix_metadata.json"
    model_file = adapter_path / "adapter_model.safetensors"
    
    print(f"ğŸ“ Adapter directory: {adapter_path}")
    print(f"   Config file: {'âœ…' if config_file.exists() else 'âŒ'} {config_file}")
    print(f"   Metadata file: {'âœ…' if metadata_file.exists() else 'âŒ'} {metadata_file}")
    print(f"   Model file: {'âœ…' if model_file.exists() else 'âŒ'} {model_file}")
    
    adapter_info = {}
    
    # Load configuration
    if config_file.exists():
        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
            
            print(f"\nğŸ“‹ ADAPTER CONFIGURATION:")
            print(f"   Base Model: {config.get('base_model_name_or_path', 'Unknown')}")
            print(f"   PEFT Type: {config.get('peft_type', 'Unknown')}")
            print(f"   Rank (r): {config.get('r', 'Unknown')}")
            print(f"   Alpha: {config.get('lora_alpha', 'Unknown')}")
            print(f"   Target Modules: {config.get('target_modules', [])}")
            print(f"   Dropout: {config.get('lora_dropout', 'Unknown')}")
            print(f"   Task Type: {config.get('task_type', 'Unknown')}")
            
            adapter_info['config'] = config
            
        except Exception as e:
            print(f"âŒ Failed to load config: {e}")
    
    # Load metadata
    if metadata_file.exists():
        try:
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
            
            print(f"\nğŸ“Š ADAPTER METADATA:")
            print(f"   Description: {metadata.get('description', 'No description')}")
            print(f"   Domain: {metadata.get('domain', 'Unknown')}")
            print(f"   Capabilities: {metadata.get('capabilities', [])}")
            print(f"   Training Dataset: {metadata.get('training_dataset', 'Unknown')}")
            
            adapter_info['metadata'] = metadata
            
        except Exception as e:
            print(f"âŒ Failed to load metadata: {e}")
    
    return adapter_info


def test_code_generation_tasks():
    """Test various code generation tasks with the code adapter."""
    
    print("\nğŸ§ª CODE GENERATION TESTING")
    print("=" * 60)
    
    # Comprehensive test cases for code generation
    test_cases = [
        {
            "name": "Simple Function",
            "prompt": "Write a Python function to calculate the factorial of a number",
            "expected_elements": ["def", "factorial", "return", "if", "else"],
            "difficulty": "Easy"
        },
        {
            "name": "Data Structure",
            "prompt": "Create a Python class for a binary search tree with insert and search methods",
            "expected_elements": ["class", "def __init__", "def insert", "def search", "self"],
            "difficulty": "Medium"
        },
        {
            "name": "Algorithm Implementation",
            "prompt": "Implement quicksort algorithm in Python with comments explaining each step",
            "expected_elements": ["def quicksort", "partition", "pivot", "recursion", "#"],
            "difficulty": "Medium"
        },
        {
            "name": "Web API",
            "prompt": "Create a Flask REST API endpoint that accepts JSON data and returns a response",
            "expected_elements": ["from flask", "@app.route", "request.json", "jsonify"],
            "difficulty": "Medium"
        },
        {
            "name": "Database Query",
            "prompt": "Write a SQL query to find the top 5 customers by total order value",
            "expected_elements": ["SELECT", "JOIN", "GROUP BY", "ORDER BY", "LIMIT"],
            "difficulty": "Easy"
        },
        {
            "name": "Error Handling",
            "prompt": "Write a Python function that reads a file and handles potential errors gracefully",
            "expected_elements": ["try:", "except:", "with open", "return"],
            "difficulty": "Medium"
        },
        {
            "name": "Complex Algorithm",
            "prompt": "Implement a function to find the longest common subsequence between two strings",
            "expected_elements": ["def", "dynamic programming", "matrix", "for", "if"],
            "difficulty": "Hard"
        },
        {
            "name": "Object-Oriented Design",
            "prompt": "Design a Python class hierarchy for different types of vehicles with inheritance",
            "expected_elements": ["class Vehicle", "class Car", "def __init__", "super()", "inheritance"],
            "difficulty": "Medium"
        }
    ]
    
    try:
        from src.core.modular_engine import ModularAdaptrixEngine
        
        print("ğŸš€ Initializing Adaptrix with Qwen3-1.7B...")
        engine = ModularAdaptrixEngine(
            model_id="Qwen/Qwen3-1.7B",
            device="cpu",
            adapters_dir="adapters"
        )
        
        if not engine.initialize():
            print("âŒ Failed to initialize engine")
            return False
        
        print("âœ… Engine initialized successfully!")
        
        # Check if code adapter is available
        available_adapters = engine.list_adapters()
        print(f"ğŸ“¦ Available adapters: {available_adapters}")
        
        if "code" not in available_adapters:
            print("âŒ Code adapter not found in available adapters")
            return False
        
        # Load the code adapter
        print("\nğŸ”Œ Loading code adapter...")
        if not engine.load_adapter("code"):
            print("âŒ Failed to load code adapter")
            return False
        
        print("âœ… Code adapter loaded successfully!")
        
        # Test each case
        results = []
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nğŸ§ª Test {i}: {test_case['name']} ({test_case['difficulty']})")
            print("-" * 50)
            print(f"ğŸ“ Prompt: {test_case['prompt']}")
            
            start_time = time.time()
            
            # Generate with code-specific parameters
            response = engine.generate(
                test_case['prompt'],
                task_type="code",
                max_length=512,
                temperature=0.3,  # Lower temperature for more deterministic code
                top_p=0.95
            )
            
            generation_time = time.time() - start_time
            
            print(f"â±ï¸ Generated in {generation_time:.2f}s")
            print(f"ğŸ“ Response length: {len(response)} characters")
            
            # Evaluate response quality
            quality_score = evaluate_code_response(response, test_case['expected_elements'])
            
            print(f"ğŸ“Š Quality Score: {quality_score:.1f}%")
            
            # Show response
            print(f"\nğŸ’» GENERATED CODE:")
            print("```python")
            print(response[:800] + ("..." if len(response) > 800 else ""))
            print("```")
            
            # Store results
            results.append({
                'name': test_case['name'],
                'difficulty': test_case['difficulty'],
                'prompt': test_case['prompt'],
                'response': response,
                'quality_score': quality_score,
                'generation_time': generation_time,
                'response_length': len(response)
            })
        
        # Generate comprehensive report
        generate_code_adapter_report(results)
        
        # Cleanup
        engine.unload_adapter("code")
        engine.cleanup()
        
        return True
        
    except Exception as e:
        print(f"âŒ Testing failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def evaluate_code_response(response: str, expected_elements: list) -> float:
    """Evaluate the quality of a code generation response."""
    
    score = 0
    max_score = 100
    
    # Check for expected elements (40 points)
    elements_found = 0
    for element in expected_elements:
        if element.lower() in response.lower():
            elements_found += 1
    
    element_score = (elements_found / len(expected_elements)) * 40 if expected_elements else 0
    score += element_score
    
    # Check for code structure (20 points)
    structure_indicators = ['def ', 'class ', 'import ', 'from ', 'return', 'if ', 'for ', 'while ']
    structure_found = sum(1 for indicator in structure_indicators if indicator in response)
    structure_score = min(structure_found * 3, 20)
    score += structure_score
    
    # Check for proper formatting (15 points)
    formatting_score = 0
    if '```' in response or 'def ' in response:
        formatting_score += 5
    if ':' in response and '\n' in response:
        formatting_score += 5
    if any(comment in response for comment in ['#', '"""', "'''"]):
        formatting_score += 5
    score += formatting_score
    
    # Check response length (10 points)
    if len(response) >= 100:
        length_score = min(len(response) / 50, 10)
    else:
        length_score = len(response) / 10
    score += length_score
    
    # Check for completeness (15 points)
    completeness_score = 0
    if response.count('def ') >= 1:
        completeness_score += 5
    if 'return' in response:
        completeness_score += 5
    if len(response.split('\n')) >= 5:
        completeness_score += 5
    score += completeness_score
    
    return min(score, max_score)


def test_code_adapter_vs_baseline():
    """Compare code adapter performance vs baseline model."""
    
    print("\nğŸ“Š CODE ADAPTER VS BASELINE COMPARISON")
    print("=" * 60)
    
    comparison_prompts = [
        "Write a Python function to reverse a string",
        "Create a simple calculator class in Python",
        "Implement bubble sort algorithm"
    ]
    
    try:
        from src.core.modular_engine import ModularAdaptrixEngine
        
        engine = ModularAdaptrixEngine("Qwen/Qwen3-1.7B", "cpu", "adapters")
        
        if not engine.initialize():
            print("âŒ Failed to initialize engine")
            return
        
        for prompt in comparison_prompts:
            print(f"\nğŸ§ª Testing: {prompt}")
            print("-" * 40)
            
            # Test without adapter (baseline)
            print("ğŸ“Š Baseline (No Adapter):")
            baseline_response = engine.generate(prompt, task_type="code", max_length=300)
            baseline_score = evaluate_code_response(baseline_response, ["def", "return"])
            print(f"   Quality: {baseline_score:.1f}%")
            print(f"   Length: {len(baseline_response)} chars")
            
            # Test with code adapter
            if engine.load_adapter("code"):
                print("\nğŸ”Œ With Code Adapter:")
                adapter_response = engine.generate(prompt, task_type="code", max_length=300)
                adapter_score = evaluate_code_response(adapter_response, ["def", "return"])
                print(f"   Quality: {adapter_score:.1f}%")
                print(f"   Length: {len(adapter_response)} chars")
                
                improvement = adapter_score - baseline_score
                print(f"   ğŸ“ˆ Improvement: {improvement:+.1f}%")
                
                engine.unload_adapter("code")
            else:
                print("âŒ Failed to load code adapter")
        
        engine.cleanup()
        
    except Exception as e:
        print(f"âŒ Comparison failed: {e}")


def generate_code_adapter_report(results: list):
    """Generate comprehensive report for code adapter testing."""
    
    print(f"\nğŸ“Š CODE ADAPTER COMPREHENSIVE REPORT")
    print("=" * 80)
    
    if not results:
        print("âŒ No results to report")
        return
    
    # Calculate statistics
    total_tests = len(results)
    avg_quality = sum(r['quality_score'] for r in results) / total_tests
    avg_time = sum(r['generation_time'] for r in results) / total_tests
    avg_length = sum(r['response_length'] for r in results) / total_tests
    
    # Quality distribution
    excellent = sum(1 for r in results if r['quality_score'] >= 80)
    good = sum(1 for r in results if 60 <= r['quality_score'] < 80)
    fair = sum(1 for r in results if 40 <= r['quality_score'] < 60)
    poor = sum(1 for r in results if r['quality_score'] < 40)
    
    print(f"\nğŸ“ˆ OVERALL PERFORMANCE:")
    print(f"   Total Tests: {total_tests}")
    print(f"   Average Quality: {avg_quality:.1f}%")
    print(f"   Average Generation Time: {avg_time:.2f}s")
    print(f"   Average Response Length: {avg_length:.0f} characters")
    
    print(f"\nğŸ“Š QUALITY DISTRIBUTION:")
    print(f"   Excellent (80%+): {excellent} tests ({excellent/total_tests*100:.1f}%)")
    print(f"   Good (60-79%): {good} tests ({good/total_tests*100:.1f}%)")
    print(f"   Fair (40-59%): {fair} tests ({fair/total_tests*100:.1f}%)")
    print(f"   Poor (<40%): {poor} tests ({poor/total_tests*100:.1f}%)")
    
    print(f"\nğŸ“‹ DETAILED RESULTS:")
    for result in results:
        status = "ğŸŸ¢" if result['quality_score'] >= 80 else "ğŸŸ¡" if result['quality_score'] >= 60 else "ğŸ”´"
        print(f"   {status} {result['name']}: {result['quality_score']:.1f}% ({result['difficulty']})")
    
    # Performance by difficulty
    difficulties = {}
    for result in results:
        diff = result['difficulty']
        if diff not in difficulties:
            difficulties[diff] = []
        difficulties[diff].append(result['quality_score'])
    
    print(f"\nğŸ“Š PERFORMANCE BY DIFFICULTY:")
    for difficulty, scores in difficulties.items():
        avg_score = sum(scores) / len(scores)
        print(f"   {difficulty}: {avg_score:.1f}% (n={len(scores)})")
    
    # Overall assessment
    print(f"\nğŸ¯ CODE ADAPTER ASSESSMENT:")
    if avg_quality >= 80:
        print(f"   ğŸŠ EXCELLENT: Code adapter performs exceptionally well!")
        print(f"   âœ… Ready for production code generation tasks")
    elif avg_quality >= 70:
        print(f"   âœ… GOOD: Code adapter shows strong performance")
        print(f"   ğŸ”§ Minor optimizations could improve results")
    elif avg_quality >= 60:
        print(f"   âš ï¸ FAIR: Code adapter has decent performance")
        print(f"   ğŸ”§ Recommend additional training or parameter tuning")
    else:
        print(f"   âŒ POOR: Code adapter needs significant improvement")
        print(f"   ğŸ”§ Consider retraining with better data or parameters")
    
    # Save detailed report
    report_path = "code_adapter_test_report.json"
    with open(report_path, 'w') as f:
        json.dump({
            'summary': {
                'total_tests': total_tests,
                'avg_quality': avg_quality,
                'avg_time': avg_time,
                'avg_length': avg_length,
                'quality_distribution': {
                    'excellent': excellent,
                    'good': good,
                    'fair': fair,
                    'poor': poor
                }
            },
            'detailed_results': results
        }, f, indent=2)
    
    print(f"\nğŸ“„ Detailed report saved to: {report_path}")


def main():
    """Main testing function."""
    
    print("ğŸ§ª" * 80)
    print("ğŸ§ª CODE ADAPTER COMPREHENSIVE TESTING ğŸ§ª")
    print("ğŸ§ª" * 80)
    
    # Inspect adapter
    adapter_info = inspect_code_adapter()
    
    if adapter_info:
        # Test code generation
        success = test_code_generation_tasks()
        
        # Compare with baseline
        test_code_adapter_vs_baseline()
        
        if success:
            print(f"\nğŸŠ CODE ADAPTER TESTING COMPLETE!")
        else:
            print(f"\nâŒ Testing encountered issues")
    else:
        print(f"\nâŒ Could not inspect code adapter")


if __name__ == "__main__":
    main()
