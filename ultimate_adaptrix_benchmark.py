#!/usr/bin/env python3
"""
üöÄ ULTIMATE ADAPTRIX BENCHMARK SYSTEM üöÄ

This benchmark properly tests your manually trained adapter with:
1. Base Qwen3-1.7B model
2. Qwen3 with your trained LoRA adapter (normal PEFT)
3. Qwen3 with Adaptrix middle-layer injection (using redistributed weights)
4. Gemini Flash 2.0 API

Fixed to handle weight extraction and middle-layer redistribution properly.
"""

import sys
import os
import time
import json
import torch
import traceback
from datetime import datetime
from typing import Dict, List, Any, Optional
import google.generativeai as genai
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from safetensors import safe_open

# Add project root to path
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from src.core.engine import AdaptrixEngine

# Configure Gemini API
GEMINI_API_KEY = "AIzaSyA6qd-9dfBEDHoAk_1gStXHxs_Kg-J1cHw"
genai.configure(api_key=GEMINI_API_KEY)

class UltimateAdaptrixBenchmark:
    """
    Ultimate benchmark system for comprehensive Adaptrix evaluation.
    """
    
    def __init__(self):
        """Initialize the benchmark system."""
        self.model_name = "Qwen/Qwen3-1.7B"
        self.adapter_path = "adapters/code_adapter"
        self.device = "cpu"
        
        # Comprehensive test prompts
        self.test_prompts = [
            {
                "name": "Simple Function",
                "prompt": "Write a Python function to calculate factorial using recursion:",
                "expected": ["def", "factorial", "return", "if"]
            },
            {
                "name": "Algorithm Implementation",
                "prompt": "Create a Python function that implements binary search:",
                "expected": ["def", "binary", "search", "while", "mid"]
            },
            {
                "name": "Data Structure",
                "prompt": "Write a Python class for a simple linked list with add and find methods:",
                "expected": ["class", "LinkedList", "def", "add", "find"]
            },
            {
                "name": "String Processing",
                "prompt": "Create a function that finds all palindromes in a string:",
                "expected": ["def", "palindrome", "string", "return"]
            },
            {
                "name": "Error Handling",
                "prompt": "Write a function that safely divides two numbers with error handling:",
                "expected": ["def", "try", "except", "return"]
            }
        ]
        
        # Results storage
        self.results = {}
    
    def setup_models(self):
        """Setup all models for testing."""
        print("üöÄ Setting up models for ultimate benchmark...")
        
        # 1. Setup Base Model
        print("   Setting up Base Qwen3-1.7B model...")
        try:
            self.base_tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
            self.base_model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float32,
                device_map=self.device,
                trust_remote_code=True
            )
            if self.base_tokenizer.pad_token is None:
                self.base_tokenizer.pad_token = self.base_tokenizer.eos_token
            print("   ‚úÖ Base model ready")
        except Exception as e:
            print(f"   ‚ùå Base model setup failed: {e}")
            return False
        
        # 2. Setup Normal LoRA Model (Your trained adapter)
        print("   Setting up your trained LoRA model...")
        try:
            self.lora_model = PeftModel.from_pretrained(self.base_model, self.adapter_path)
            self.lora_tokenizer = self.base_tokenizer
            print("   ‚úÖ Your trained LoRA model ready")
        except Exception as e:
            print(f"   ‚ùå LoRA model setup failed: {e}")
            return False
        
        # 3. Setup Adaptrix Engine
        print("   Setting up Adaptrix Engine...")
        try:
            self.adaptrix_engine = AdaptrixEngine(self.model_name, self.device)
            success = self.adaptrix_engine.initialize()
            if not success:
                raise Exception("Failed to initialize Adaptrix engine")
            print("   ‚úÖ Adaptrix Engine ready")
        except Exception as e:
            print(f"   ‚ùå Adaptrix setup failed: {e}")
            return False
        
        # 4. Setup Gemini Model
        print("   Setting up Gemini Flash 2.0...")
        try:
            self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
            print("   ‚úÖ Gemini model ready")
        except Exception as e:
            print(f"   ‚ùå Gemini setup failed: {e}")
            return False
        
        return True
    
    def extract_and_redistribute_weights(self):
        """Extract weights from your LoRA and redistribute for middle layers."""
        print("üîÑ Extracting and redistributing your LoRA weights for middle layers...")
        
        try:
            import shutil
            
            # Create Adaptrix adapter directory
            adaptrix_dir = "adapters/ultimate_adaptrix"
            if os.path.exists(adaptrix_dir):
                shutil.rmtree(adaptrix_dir)
            os.makedirs(adaptrix_dir)
            
            # Create metadata
            metadata = {
                'name': 'ultimate_adaptrix',
                'version': '1.0.0',
                'description': 'Middle-layer injection using redistributed weights from your trained LoRA',
                'source': 'redistributed_trained_lora',
                'base_model': self.model_name,
                'target_layers': [9, 14, 19],  # Middle layers for 28-layer model
                'target_modules': ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj'],
                'rank': 8,
                'alpha': 32,
                'training_steps': 4600,
                'redistribution_method': 'layer_averaging'
            }
            
            # Save metadata
            with open(os.path.join(adaptrix_dir, "metadata.json"), 'w') as f:
                json.dump(metadata, f, indent=2)
            
            # Load original adapter weights
            adapter_file = os.path.join(self.adapter_path, "adapter_model.safetensors")
            
            with safe_open(adapter_file, framework="pt", device="cpu") as f:
                print(f"   üìä Available keys in adapter: {len(list(f.keys()))}")
                
                # Get all available keys
                all_keys = list(f.keys())
                
                # Find layer patterns in your trained adapter
                layer_modules = {}
                for key in all_keys:
                    if "lora_A" in key or "lora_B" in key:
                        parts = key.split('.')
                        for i, part in enumerate(parts):
                            if part == "layers" and i + 1 < len(parts):
                                try:
                                    layer_num = int(parts[i + 1])
                                    if layer_num not in layer_modules:
                                        layer_modules[layer_num] = []
                                    layer_modules[layer_num].append(key)
                                    break
                                except ValueError:
                                    continue
                
                print(f"   üìã Found weights for layers: {sorted(layer_modules.keys())}")
                
                # Redistribute weights to middle layers
                target_layers = [9, 14, 19]
                source_layers = sorted(layer_modules.keys())
                
                for target_layer in target_layers:
                    layer_weights = {}
                    
                    # For each target module
                    for module_name in ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj']:
                        
                        # Collect weights from multiple source layers and average them
                        lora_a_weights = []
                        lora_b_weights = []
                        
                        for source_layer in source_layers[:5]:  # Use first 5 layers as source
                            for key in layer_modules.get(source_layer, []):
                                if module_name in key:
                                    if "lora_A" in key:
                                        lora_a_weights.append(f.get_tensor(key))
                                    elif "lora_B" in key:
                                        lora_b_weights.append(f.get_tensor(key))
                        
                        # Average the weights if we found them
                        if lora_a_weights and lora_b_weights:
                            avg_lora_a = torch.stack(lora_a_weights).mean(dim=0)
                            avg_lora_b = torch.stack(lora_b_weights).mean(dim=0)
                            
                            layer_weights[module_name] = {
                                'lora_A': avg_lora_a * 0.5,  # Scale down for stability
                                'lora_B': avg_lora_b * 0.5,
                                'rank': 8,
                                'alpha': 16  # Reduced alpha for middle layer injection
                            }
                            
                            print(f"   ‚úÖ Redistributed {module_name} to layer {target_layer} (A: {avg_lora_a.shape}, B: {avg_lora_b.shape})")
                    
                    # Save layer weights
                    if layer_weights:
                        layer_file = os.path.join(adaptrix_dir, f"layer_{target_layer}.pt")
                        torch.save(layer_weights, layer_file)
                        print(f"   üíæ Saved layer {target_layer} with {len(layer_weights)} modules")
                    else:
                        print(f"   ‚ö†Ô∏è  No weights found for layer {target_layer}")
            
            print("   ‚úÖ Weight redistribution completed successfully")
            return True
            
        except Exception as e:
            print(f"   ‚ùå Weight redistribution failed: {e}")
            traceback.print_exc()
            return False
    
    def generate_response(self, model_type: str, prompt: str, max_length: int = 120) -> Dict[str, Any]:
        """Generate response from specified model type."""
        start_time = time.time()
        
        try:
            if model_type == "base":
                # Base model generation
                inputs = self.base_tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                
                with torch.no_grad():
                    outputs = self.base_model.generate(
                        **inputs,
                        max_length=inputs['input_ids'].shape[1] + max_length,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=self.base_tokenizer.eos_token_id,
                        early_stopping=True
                    )
                
                response = self.base_tokenizer.decode(outputs[0], skip_special_tokens=True)
                response = response[len(prompt):].strip()
                
            elif model_type == "lora":
                # Your trained LoRA model
                inputs = self.lora_tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                
                with torch.no_grad():
                    outputs = self.lora_model.generate(
                        **inputs,
                        max_length=inputs['input_ids'].shape[1] + max_length,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=self.lora_tokenizer.eos_token_id,
                        early_stopping=True
                    )
                
                response = self.lora_tokenizer.decode(outputs[0], skip_special_tokens=True)
                response = response[len(prompt):].strip()
                
            elif model_type == "adaptrix":
                # Adaptrix with middle-layer injection
                success = self.adaptrix_engine.load_adapter("ultimate_adaptrix")
                if not success:
                    raise Exception("Failed to load Ultimate Adaptrix adapter")
                
                response = self.adaptrix_engine.generate(prompt, max_length=max_length)
                
            elif model_type == "gemini":
                # Gemini Flash 2.0
                response = self.gemini_model.generate_content(prompt)
                response = response.text
                
            else:
                raise ValueError(f"Unknown model type: {model_type}")
            
            generation_time = time.time() - start_time
            
            return {
                "content": response,
                "generation_time": generation_time,
                "length": len(response),
                "success": True
            }
            
        except Exception as e:
            return {
                "content": f"Error: {str(e)}",
                "generation_time": time.time() - start_time,
                "length": 0,
                "success": False
            }
    
    def evaluate_response_quality(self, response: str, expected_elements: List[str]) -> float:
        """Evaluate response quality."""
        if not response or "Error:" in response:
            return 0.0
        
        # Check for expected elements
        elements_found = sum(1 for element in expected_elements if element.lower() in response.lower())
        completeness_score = (elements_found / len(expected_elements)) * 100
        
        # Check for code structure
        has_function = "def " in response
        has_class = "class " in response
        has_return = "return " in response
        has_logic = any(keyword in response for keyword in ["if ", "for ", "while ", "try "])
        has_code_block = "```" in response or has_function or has_class
        
        structure_score = 0
        if has_function or has_class:
            structure_score += 40
        if has_return:
            structure_score += 30
        if has_logic:
            structure_score += 20
        if has_code_block:
            structure_score += 10
        
        # Final score
        return min(100, (completeness_score * 0.6 + structure_score * 0.4))
    
    def run_ultimate_benchmark(self):
        """Run the ultimate comprehensive benchmark."""
        print("üöÄ STARTING ULTIMATE ADAPTRIX BENCHMARK")
        print("=" * 80)
        
        # Setup
        if not self.setup_models():
            print("‚ùå Model setup failed. Exiting.")
            return
        
        # Extract and redistribute weights
        if not self.extract_and_redistribute_weights():
            print("‚ùå Weight redistribution failed. Exiting.")
            return
        
        # Test all models
        models = {
            "base": "Base Qwen3-1.7B",
            "lora": "Qwen3 + Your Trained LoRA",
            "adaptrix": "Qwen3 + Ultimate Adaptrix",
            "gemini": "Gemini Flash 2.0"
        }
        
        all_results = {}
        
        for test in self.test_prompts:
            print(f"\nüìä Testing: {test['name']}")
            print("-" * 50)
            
            test_results = {}
            
            for model_key, model_name in models.items():
                print(f"   üîç {model_name}...")
                
                result = self.generate_response(model_key, test['prompt'])
                quality = self.evaluate_response_quality(result['content'], test['expected'])
                
                result['quality_score'] = quality
                test_results[model_key] = result
                
                print(f"      Score: {quality:.1f}/100 | Time: {result['generation_time']:.2f}s")
                preview = result['content'][:80].replace('\n', ' ')
                print(f"      Preview: {preview}...")
                
                # Small delay
                time.sleep(0.5)
            
            all_results[test['name']] = test_results
        
        # Generate comprehensive report
        self.generate_ultimate_report(all_results, models)
    
    def generate_ultimate_report(self, all_results: Dict, models: Dict):
        """Generate the ultimate comprehensive report."""
        print("\n" + "=" * 80)
        print("üèÜ ULTIMATE ADAPTRIX BENCHMARK RESULTS")
        print("=" * 80)
        
        # Calculate overall statistics
        overall_stats = {}
        
        for model_key, model_name in models.items():
            scores = []
            times = []
            success_count = 0
            
            for test_name, test_results in all_results.items():
                if model_key in test_results:
                    result = test_results[model_key]
                    if result['success']:
                        scores.append(result['quality_score'])
                        times.append(result['generation_time'])
                        success_count += 1
            
            overall_stats[model_key] = {
                "name": model_name,
                "avg_score": sum(scores) / len(scores) if scores else 0,
                "avg_time": sum(times) / len(times) if times else 0,
                "success_rate": success_count / len(self.test_prompts),
                "total_tests": len(self.test_prompts)
            }
        
        # Print rankings
        print("\nüèÜ OVERALL PERFORMANCE RANKINGS")
        print("-" * 60)
        
        sorted_models = sorted(overall_stats.items(), key=lambda x: x[1]['avg_score'], reverse=True)
        
        for rank, (model_key, stats) in enumerate(sorted_models, 1):
            print(f"{rank}. {stats['name']}")
            print(f"   Average Score: {stats['avg_score']:.1f}/100")
            print(f"   Average Time: {stats['avg_time']:.2f}s")
            print(f"   Success Rate: {stats['success_rate']:.1%}")
            print()
        
        # Detailed analysis
        print("üîç DETAILED ANALYSIS")
        print("-" * 60)
        
        base_score = overall_stats['base']['avg_score']
        lora_score = overall_stats['lora']['avg_score']
        adaptrix_score = overall_stats['adaptrix']['avg_score']
        gemini_score = overall_stats['gemini']['avg_score']
        
        print(f"‚Ä¢ Your LoRA vs Base Model: {((lora_score - base_score) / base_score * 100):+.1f}% improvement")
        if adaptrix_score > 0:
            print(f"‚Ä¢ Adaptrix vs Base Model: {((adaptrix_score - base_score) / base_score * 100):+.1f}% improvement")
            print(f"‚Ä¢ Adaptrix vs Your LoRA: {((adaptrix_score - lora_score) / lora_score * 100):+.1f}% difference")
            print(f"‚Ä¢ Adaptrix vs Gemini: {((adaptrix_score - gemini_score) / gemini_score * 100):+.1f}% difference")
        else:
            print("‚Ä¢ Adaptrix: Failed to load - check weight redistribution")
        
        # Speed comparison
        print(f"\n‚ö° SPEED COMPARISON")
        print("-" * 60)
        for model_key, stats in overall_stats.items():
            if stats['success_rate'] > 0:
                print(f"‚Ä¢ {stats['name']}: {stats['avg_time']:.2f}s")
        
        # Save results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"ultimate_benchmark_results_{timestamp}.json"
        
        full_report = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "model_name": self.model_name,
                "adapter_path": self.adapter_path,
                "total_tests": len(self.test_prompts),
                "weight_redistribution": "middle_layer_averaging"
            },
            "overall_statistics": overall_stats,
            "detailed_results": all_results,
            "insights": {
                "lora_vs_base": ((lora_score - base_score) / base_score * 100) if base_score > 0 else 0,
                "adaptrix_vs_base": ((adaptrix_score - base_score) / base_score * 100) if base_score > 0 and adaptrix_score > 0 else 0,
                "adaptrix_vs_lora": ((adaptrix_score - lora_score) / lora_score * 100) if lora_score > 0 and adaptrix_score > 0 else 0,
                "adaptrix_vs_gemini": ((adaptrix_score - gemini_score) / gemini_score * 100) if gemini_score > 0 and adaptrix_score > 0 else 0
            }
        }
        
        with open(results_file, 'w') as f:
            json.dump(full_report, f, indent=2)
        
        print(f"\nüíæ Detailed results saved to: {results_file}")
        
        # Final verdict
        print("\nüéØ ULTIMATE VERDICT")
        print("-" * 60)
        
        # Your LoRA performance
        if lora_score > base_score:
            improvement = ((lora_score - base_score) / base_score * 100)
            print(f"üéâ YOUR MANUALLY TRAINED LORA: EXCELLENT!")
            print(f"   {improvement:.1f}% improvement over base model")
            
            if lora_score > gemini_score * 0.9:
                print(f"   üèÜ Achieves {(lora_score/gemini_score*100):.1f}% of Gemini performance!")
                print("   Outstanding result for a local small model!")
            else:
                print(f"   üëç Achieves {(lora_score/gemini_score*100):.1f}% of Gemini performance")
                print("   Solid performance for a 1.7B parameter model!")
        
        # Middle-layer injection verdict
        if adaptrix_score > 0:
            if adaptrix_score > lora_score * 1.05:
                improvement = ((adaptrix_score - lora_score) / lora_score * 100)
                print(f"\n‚úÖ MIDDLE-LAYER INJECTION: SUCCESS!")
                print(f"   {improvement:.1f}% improvement over normal LoRA")
                print("   üéä Middle-layer injection proves effective!")
            elif adaptrix_score > lora_score * 0.95:
                print(f"\n‚öñÔ∏è  MIDDLE-LAYER INJECTION: COMPARABLE")
                print(f"   Performance within 5% of normal LoRA")
                print("   ü§î Shows promise but needs optimization")
            else:
                difference = ((lora_score - adaptrix_score) / adaptrix_score * 100)
                print(f"\n‚ùå MIDDLE-LAYER INJECTION: NEEDS WORK")
                print(f"   Normal LoRA performs {difference:.1f}% better")
                print("   üîß Strategy needs refinement")
        else:
            print(f"\n‚ùå MIDDLE-LAYER INJECTION: FAILED TO LOAD")
            print("   Check weight redistribution and injection mechanism")
        
        print("\nüöÄ ULTIMATE BENCHMARK COMPLETE! üöÄ")
        print("Congratulations on training an excellent adapter!")


def main():
    """Run the ultimate benchmark."""
    benchmark = UltimateAdaptrixBenchmark()
    benchmark.run_ultimate_benchmark()


if __name__ == "__main__":
    main() 